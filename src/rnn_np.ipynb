{
 "metadata": {
  "name": "",
  "signature": "sha256:a41c05863f3fae8716fcd2bfcbfc5a5a837612cc1c663e9c23d1e1967cced10f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "# Numeric Python\n",
      "import numpy as np\n",
      "\n",
      "# Natural Language ToolKit\n",
      "import nltk\n",
      "\n",
      "# System Libraries\n",
      "import csv\n",
      "import itertools\n",
      "import os\n",
      "import sys\n",
      "import time\n",
      "import operator\n",
      "from datetime import datetime\n",
      "\n",
      "from datasets import load_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Tanh activation function\n",
      "def tanh(z):\n",
      "    return np.tanh(z)\n",
      "\n",
      "# Softmax function\n",
      "def softmax(z):\n",
      "    zt = np.exp(z - np.max(z))\n",
      "    return zt/ np.sum(zt)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class RNNnp:\n",
      "    # Initializes a Recurrent Neural Network with the provided parameters\n",
      "    def __init__(self, word_dim=8000, hidden_dim=100, bptt_truncate=4):\n",
      "        '''\n",
      "            word_dim   - Vocabulary Size\n",
      "            hidden_dim - Dimension (number of units) of the hidden layer\n",
      "            \n",
      "        '''\n",
      "        # Helper Variables\n",
      "        self.word_dim = word_dim\n",
      "        self.hidden_dim = hidden_dim\n",
      "        self.bptt_truncate = bptt_truncate\n",
      "        \n",
      "        '''\n",
      "            U - Weights(Matrix) from input state  -> hidden state\n",
      "            V - Weights(Matrix) from hidden state -> output state\n",
      "            W - Weights(Matrix) from hidden state -> hidden state (next)\n",
      "        '''\n",
      "        # Randomly Initialize the parameters\n",
      "        self.U = np.random.rand(hidden_dim, word_dim)/np.sqrt(word_dim)\n",
      "        self.V = np.random.rand(word_dim, hidden_dim)/np.sqrt(hidden_dim)\n",
      "        self.W = np.random.rand(hidden_dim, hidden_dim)/np.sqrt(hidden_dim)\n",
      "    \n",
      "    def forward_propagation(self, x):\n",
      "        '''\n",
      "            Defines the forward propagation of the net\n",
      "        '''\n",
      "        # Number of timesteps\n",
      "        timesteps = len(x)\n",
      "        # Save all the hidden states, since we need them later\n",
      "        # Add an additional element for the initial hidden unit and set it to '0'\n",
      "        hs = np.zeros((timesteps +1 , self.hidden_dim))\n",
      "        hs[-1] = np.zeros(self.hidden_dim)\n",
      "        \n",
      "        # Again save the outputs at each time step\n",
      "        output = np.zeros((timesteps, self.word_dim))\n",
      "        \n",
      "        # Now propogate the net for each time step\n",
      "        for timestep in  xrange(timesteps):\n",
      "            # Indexing U by x[t]\n",
      "            hs[timestep] = tanh(self.U[:, x[timestep]]+ self.W.dot(hs[timestep-1]))\n",
      "            output[timestep] = softmax(self.V.dot(hs[timestep]))\n",
      "        return [output, hs]\n",
      "    \n",
      "    def predict(self, x):\n",
      "        '''\n",
      "            Perform forward propagation and returns the index of highest score\n",
      "        '''\n",
      "        output, hs = forward_propagation(x)\n",
      "        return np.argmax(output, axis=1)\n",
      "    \n",
      "    def calculate_total_loss(self, x, y):\n",
      "        '''\n",
      "            Calculates the total loss for the network predictions\n",
      "        '''\n",
      "        loss = 0\n",
      "        # For each sentence\n",
      "        for index in xrange(len(y)):\n",
      "            output, hs = self.forward_propagation(x[index])\n",
      "            # Check the 'correct' words\n",
      "            error_words = output[xrange(len(y[index])), y[index]]\n",
      "            # Add it to loss 'entropy loss'\n",
      "            loss += -1 * np.sum(np.log(error_words))\n",
      "        return loss\n",
      "    \n",
      "    def calculate_loss(self, x, y):\n",
      "        # Divides the total loss by number of training examples\n",
      "        loss = self.calculate_total_loss(x, y)\n",
      "        N = np.sum((len(y_i) for y_i in y))\n",
      "        return loss/N\n",
      "    \n",
      "    def bptt(self, x, y):\n",
      "        Timesteps = len(y)\n",
      "        # Perform Forward Propagation\n",
      "        output, hs = self.forward_propagation(x)\n",
      "        # Accumulating the Gradients w.r.t U, V and W\n",
      "        nablaLdU = np.zeros(self.U.shape) # Partial derivate of cost(L) w.r.t U\n",
      "        nablaLdV = np.zeros(self.V.shape) # Partial derivate of cost(L) w.r.t V\n",
      "        nablaLdW = np.zeros(self.W.shape) # Partial derivate of cost(L) w.r.t W\n",
      "        \n",
      "        delta_ouptut = output # Difference in ouptut\n",
      "        # For all time steps , decrease it by 1\n",
      "        delta_ouptut[np.arange(len(y)), y] -= 1\n",
      "        # Now, Back Propogate the error\n",
      "        for timestep in range(Timesteps)[::-1]:\n",
      "            nablaLdV += np.outer(delta_ouptut[timestep], hs[timestep].T)\n",
      "            delta_t = self.V.T.dot(delta_ouptut[timestep]) *(1 - (hs[timestep]**2))\n",
      "            # Back Propagation through time (for at most bptt_truncate steps)\n",
      "            for bptt_step in range(max(0, timestep - self.bptt_truncate), timestep + 1)[::-1]:\n",
      "                # Add Gradients to previous step\n",
      "                nablaLdW += np.outer(delta_t, hs[bptt_step - 1])\n",
      "                nablaLdU[:, x[bptt_step]] += delta_t\n",
      "                # Now we update delta for the next time step \"t + 1\"\n",
      "                delta_t = self.W.T.dot(delta_t) * (1 - (hs[bptt_step - 1] ** 2))\n",
      "        return [nablaLdU, nablaLdV, nablaLdW]\n",
      "    \n",
      "    def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
      "        # Calculate the gradients using backpropagation through time\n",
      "        bptt_grads = self.bptt(x, y)\n",
      "        # Define the parameters for the model, we want to check\n",
      "        model_parameters =[\"U\", \"V\", \"W\"]\n",
      "        # Check for every parameters\n",
      "        for param_index, param_name in enumerate(model_parameters):\n",
      "            # Get the actual value from the model\n",
      "            param = operator.attrgetter(param_name)(self)\n",
      "            print 'Checking Gradients for parameter:',param_name,'with shape',param.shape\n",
      "            # Iterating over each parameter of the model\n",
      "            # \u201cmulti_index\u201d causes a multi-index, or a tuple of indices with one per iteration dimension, to be tracked.\n",
      "            # \u201creadwrite\u201d indicates the operand will be read from and written to.\n",
      "            itr = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n",
      "            while not itr.finished:\n",
      "                ml_index = itr.multi_index\n",
      "                # Save the original value, we can reset it later\n",
      "                orig_val = param[ml_index]\n",
      "                # Estimate the gradident (f(x+h)- f(x-h))/(2*h)\n",
      "                param[ml_index] = orig_val + h\n",
      "                grad_plus = self.calculate_total_loss([x], [y])\n",
      "                param[ml_index] = orig_val - h\n",
      "                grad_minus = self.calculate_total_loss([x], [y])\n",
      "                estimated_grad = (grad_plus - grad_minus) / (2*h)\n",
      "                # Reset the parameter to the original value\n",
      "                param[ml_index] = orig_val\n",
      "                backprop_grad = bptt_grads[param_index][ml_index]\n",
      "                # Now, we calculate the relative error: (|a - b|/(|a| + |b|))\n",
      "                relative_error = np.abs(backprop_grad - estimated_grad)/(np.abs(backprop_grad) + np.abs(estimated_grad))\n",
      "                # If the error is too high, then backpropgation is wrong,so fail the graident check\n",
      "                if relative_error > error_threshold:\n",
      "                    print 'Gradient Checking for parameter',param_name,'index',ml_index\n",
      "                    print 'Positive loss(f(x+h):',grad_plus\n",
      "                    print 'Negative loss(f(x-h):',grad_minus\n",
      "                    print 'Estimated Gradient:',estimated_grad\n",
      "                    print 'Back Propagation Gradient:',backprop_grad\n",
      "                    print 'Relative Error:',relative_error\n",
      "                    return\n",
      "                itr.iternext()\n",
      "            print 'Gradient Check for parameter',param_name,'passed'\n",
      "        \n",
      "    # Perform a single step of SGD\n",
      "    def sgd_step_np(self, x, y, lr=0.01):\n",
      "        # Calculate the gradients\n",
      "        nablaLdU, nablaLdV, nablaLdW = self.bptt(x, y)\n",
      "        # Changing the parameters w.r.t learning rate('lr')\n",
      "        self.U -= lr*nablaLdU\n",
      "        self.V -= lr*nablaLdV\n",
      "        self.W -= lr*nablaLdW\n",
      "    \n",
      "    # Stochastic Gradient Descent Optimization\n",
      "    def train_sgd_np(self, x_tr, y_tr, lr=0.01, nb_epochs=1000, validFreq=5):\n",
      "        '''\n",
      "            x_tr - Training Data Values\n",
      "            y_tr - Training Data Labels\n",
      "            lr   - Learning Rate\n",
      "            nb_epochs - Number of epochs\n",
      "            validFreq - Evaluate the model after this many epochs \n",
      "        '''\n",
      "        # Keeping track of losses, (We'll plot them later)\n",
      "        losses = []\n",
      "        # Keep track of number of sample seen\n",
      "        n_samples_seen = 0\n",
      "        for n_epoch in xrange(nb_epochs):\n",
      "            # Evaluate loss, using validFreq\n",
      "            if n_epoch%validFreq == 0:\n",
      "                loss = self.calculate_loss(x_tr, y_tr)\n",
      "                losses.append((n_samples_seen, loss))\n",
      "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
      "                print time,'Loss after',n_samples_seen,'samples and epoch',n_epoch,'is',loss\n",
      "                # if losses increases, decrease the learning rate\n",
      "                if is_loss_evaluated and loss > losses[-2][1]:\n",
      "                    lr *= 0.5\n",
      "                    print 'Decreasing learning rate to',lr\n",
      "                #sys.stdout.flush()\n",
      "                if not is_loss_evaluated:\n",
      "                    is_loss_evaluated = True\n",
      "            # for each training example, calculate the gradient\n",
      "            for index in range(len(x_tr)):\n",
      "                # Perform one step of SGD\n",
      "                self.sgd_step_np(x_tr[index], y_tr[index], lr=lr)\n",
      "                n_samples_seen +=1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 140
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_x, train_y = load_data()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Reading CSV file.. ../data/reddit-comments-2015-08.csv\n",
        "Parsed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 79170 sentences\n",
        "Found"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 65751  unique word tokens\n",
        "Using Vocabulary of size 8000\n",
        "The least frequent word in the vocabulary is \"devoted\" and appeared 10 times\n"
       ]
      }
     ],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Random Seed for reproducibility\n",
      "np.random.seed(1234)\n",
      "_VOCABULARY_SIZE = int(os.environ.get('VOCABULARY_SIZE', '8000'))\n",
      "_HIDDEN_DIM = int(os.environ.get('HIDDEN_DIM', '100'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 131
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = RNNnp(word_dim=_VOCABULARY_SIZE, hidden_dim=_HIDDEN_DIM)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 120
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "out, hs = model.forward_propagation(train_x[10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Expected Loss for random projections:',np.log(_VOCABULARY_SIZE)\n",
      "print 'Actual Loss:', model.calculate_loss(train_x[:1000], train_y[:1000])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Expected Loss for random projections: 8.98719682066\n",
        "Actual Loss: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "9.01390535568\n"
       ]
      }
     ],
     "prompt_number": 116
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grad_vocab_size = 100\n",
      "np.random.seed(1234)\n",
      "model = RNNnp(word_dim=grad_vocab_size, hidden_dim=20, bptt_truncate=100)\n",
      "model.gradient_check([1, 2, 3, 4],[2, 3, 4, 5])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Checking Gradients for parameter: U with shape (20, 100)\n",
        "Gradient Check for parameter"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " U passed\n",
        "Checking Gradients for parameter: V with shape (100, 20)\n",
        "Gradient Check for parameter"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " V passed\n",
        "Checking Gradients for parameter: W with shape (20, 20)\n",
        "Gradient Check for parameter"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " W passed\n"
       ]
      }
     ],
     "prompt_number": 137
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = RNNnp(word_dim=_VOCABULARY_SIZE, hidden_dim=_HIDDEN_DIM)\n",
      "print \"Checking time for one step of SGD Optimization for sample at index 10\"\n",
      "%timeit model.sgd_step_np(train_x[10], train_y[10], lr=0.05)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Checking time for one step of SGD Optimization for sample at index 10\n",
        "1 loops, best of 3: 260 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 138
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = RNNnp(word_dim=_VOCABULARY_SIZE, hidden_dim=_HIDDEN_DIM)\n",
      "print \"Checking SGD Optimization for 100 samples\"\n",
      "%timeit losses = model.train_sgd_np(train_x[:100], train_y[:100], lr=0.05, nb_epochs=20, validFreq=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Checking SGD Optimization for 100 samples\n",
        "2016-12-08 13:10:05"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 0 samples and epoch 0 is 8.96641483677\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2016-12-08 13:10:53 Loss after 400 samples and epoch 4 is 8.79400316482\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2016-12-08 13:11:21 Loss after 800 samples and epoch 8 is 9.739012924\n",
        "Decreasing learning rate to 0.025\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2016-12-08 13:12:15 Loss after 1200 samples and epoch 12 is 7.10026174076\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2016-12-08 13:13:05 Loss after 1600 samples and epoch 16 is 6.94746842734\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2016-12-08 13:13:45 Loss after 0 samples and epoch 0 is 6.86701832393\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2016-12-08 13:14:42 Loss after 400 samples and epoch 4 is 8.63040913045\n",
        "Decreasing learning rate to 0.025\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2016-12-08 13:15:19 Loss after 800 samples and epoch 8 is 6.97320824285\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2016-12-08 13:16:15 Loss after 1200 samples and epoch 12 is 6.90977963199\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2016-12-08 13:17:09 Loss after 1600 samples and epoch 16 is 6.88394814107\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2016-12-08 13:17:54 Loss after 0 samples and epoch 0 is 6.93514677174\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2016-12-08 13:18:39 Loss after 400 samples and epoch 4 is 8.33941818893\n",
        "Decreasing learning rate to 0.025\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2016-12-08 13:19:22 Loss after 800 samples and epoch 8 is 6.92357029875\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2016-12-08 13:20:01 Loss after 1200 samples and epoch 12 is 6.89574565464\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2016-12-08 13:20:52 Loss after 1600 samples and epoch 16 is 6.88334712835\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2016-12-08 13:21:24 Loss after 0 samples and epoch 0 is 6.87693814186\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2016-12-08 13:22:18 Loss after 400 samples and epoch 4 is 8.6456982297\n",
        "Decreasing learning rate to 0.025\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2016-12-08 13:22:58 Loss after 800 samples and epoch 8 is 7.03240817077\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2016-12-08 13:23:41 Loss after 1200 samples and epoch 12 is 6.97201265852\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2016-12-08 13:24:21 Loss after 1600 samples and epoch 16 is 6.95056206529\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 3min 32s per loop\n"
       ]
      }
     ],
     "prompt_number": 141
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}