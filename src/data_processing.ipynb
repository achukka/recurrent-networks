{
 "metadata": {
  "name": "",
  "signature": "sha256:c054499ac0f2d4a48c8643328b294bba8d43cd395917aae9488cad2f2d613014"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Numeric Python\n",
      "import numpy as np\n",
      "\n",
      "# Natural Language ToolKit\n",
      "import nltk\n",
      "\n",
      "# System Libraries\n",
      "import csv\n",
      "import itertools\n",
      "import os\n",
      "import sys\n",
      "import time\n",
      "from datetime import datetime"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Global Variables\n",
      "VOCABULARY_SIZE = 9000\n",
      "UNKNOWN_TOKEN = \"UNKNOWN_TOKEN\"\n",
      "SENTENCE_START_TOKEN = \"SENTENCE_START_TOKEN\"\n",
      "SENTENCE_END_TOKEN = \"SENTENCE_END_TOKEN\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Reads data and appends SENTENCE_START AND SENTENCE_END Tokens\n",
      "def read_data(filename=\"../data/reddit-comments-2015-08.csv\", delimiter=\",\"):\n",
      "    print 'Reading CSV file..',filename\n",
      "    fp = open(filename,'rb')\n",
      "    data = csv.reader(fp, delimiter=delimiter, skipinitialspace=True)\n",
      "    data.next()\n",
      "    # Splits the comments into sentences\n",
      "    sentences = itertools.chain(*[nltk.sent_tokenize(row[0].decode('utf-8').lower()) for row in data])\n",
      "    # Add SENTENCE_START and SENTENCE_END\n",
      "    sentences = [\"%s %s %s\" %(SENTENCE_START_TOKEN, row, SENTENCE_END_TOKEN) for row in sentences]\n",
      "    \n",
      "    print \"Parsed\",len(sentences),\"sentences\"\n",
      "    return sentences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentences = read_data()\n",
      "# Tokenize the sentences into words\n",
      "tok_sentences = np.array([nltk.word_tokenize(sentence) for sentence in sentences])\n",
      "\n",
      "# Count the word frequencies\n",
      "word_freq = nltk.FreqDist(itertools.chain(*tok_words))\n",
      "print 'Found',len(word_freq.items()),\" unique word tokens\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Reading CSV file.. ../data/reddit-comments-2015-08.csv\n",
        "Parsed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 79170 sentences\n",
        "Found"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 65751  unique word tokens\n"
       ]
      }
     ],
     "prompt_number": 111
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Getting the most common words and build index_to_word and word_to_index vectors\n",
      "vocabulary = np.array(word_freq.most_common(VOCABULARY_SIZE - 1))\n",
      "index_to_word = [row[0] for row in vocabulary]\n",
      "index_to_word.append(UNKNOWN_TOKEN)\n",
      "word_to_index = dict([(word, index) for(index,word) in enumerate(index_to_word)])\n",
      "\n",
      "print 'Using Vocabulary of size',VOCABULARY_SIZE\n",
      "print 'The least frequent word in the vocabulary is \"{0}\"'.format(vocabulary[-1][0]) +\\\n",
      "      ' and appeared',vocabulary[-1][1],'times'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using Vocabulary of size 9000\n",
        "The least frequent word in the vocabulary is \"ramsay\" and appeared 8 times\n"
       ]
      }
     ],
     "prompt_number": 108
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We replace all the words not in the vocabulary with \"UNKNOWN_TOKEN\"\n",
      "for index,sentence in enumerate(tok_sentences):\n",
      "    tok_sentences[index] = [word if word in word_to_index else UNKNOWN_TOKEN for word in sentence]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 118
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Example Sentence:',sentences[0]\n",
      "print '\\nSentence after pre-processing:', tok_sentences[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Example Sentence: SENTENCE_START_TOKEN i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END_TOKEN\n",
        "\n",
        "Sentence after pre-processing: [u'SENTENCE_START_TOKEN', u'i', u'joined', u'a', u'new', u'league', u'this', u'year', u'and', u'they', u'have', u'different', u'scoring', u'rules', u'than', u'i', u\"'m\", u'used', u'to', u'.', u'SENTENCE_END_TOKEN']\n"
       ]
      }
     ],
     "prompt_number": 119
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Example Sentence:',sentences[-2]\n",
      "print '\\nSentence after pre-processing:', tok_sentences[-2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Example Sentence: SENTENCE_START_TOKEN just got denied with poid buiu 0001 and offer code 2aar 8bilou 4uqa nkr. SENTENCE_END_TOKEN\n",
        "\n",
        "Sentence after pre-processing: [u'SENTENCE_START_TOKEN', u'just', u'got', u'denied', u'with', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', u'and', u'offer', u'code', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', u'.', u'SENTENCE_END_TOKEN']\n"
       ]
      }
     ],
     "prompt_number": 120
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Creating the train data\n",
      "train_x = np.asarray([[word_to_index[word] for word in sentence[:-1]] for sentence in tok_sentences])\n",
      "train_y = np.asarray([[word_to_index[word] for word in sentence[1:]] for sentence in tok_sentences])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 123
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from rnn_np import RNNnp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 133
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = RNNnp(word_dim=VOCABULARY_SIZE)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 134
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "out, hs = model.forward_propagation(X_train[10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "global name 'tan' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-135-d44f105863be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/home/kuroku/Desktop/mygithub/rnn/src/rnn_np.py\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtimestep\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# Indexing U by x[t]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: global name 'tan' is not defined"
       ]
      }
     ],
     "prompt_number": 135
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a[0][1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 72,
       "text": [
        "79170"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}