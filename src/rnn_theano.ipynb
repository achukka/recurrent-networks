{
 "metadata": {
  "name": "",
  "signature": "sha256:70e48ba11596e7a0f85607cbaf2b68d238577a32ffafbdeecd7ea4a13aabf48f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Numeric Python\n",
      "import numpy as np\n",
      "\n",
      "# Natural Language ToolKit\n",
      "import nltk\n",
      "\n",
      "# System Libraries\n",
      "import csv\n",
      "import itertools\n",
      "import os\n",
      "import sys\n",
      "import time\n",
      "import operator\n",
      "from datetime import datetime\n",
      "from theano import tensor\n",
      "import theano\n",
      "\n",
      "from datasets import load_data, read_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Tanh activation function\n",
      "def tanh(z):\n",
      "    return np.tanh(z)\n",
      "\n",
      "# Rectified Linear Unit\n",
      "def relu(z):\n",
      "    return np.maximum(0, z)\n",
      "\n",
      "# Softmax function\n",
      "def softmax(z):\n",
      "    zt = np.exp(z - np.max(z))\n",
      "    return zt/ np.sum(zt)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class RNNTheano:\n",
      "    # Initializes a Recurrent Neural Network with the provided parameters\n",
      "    def __init__(self, word_dim=8000, hidden_dim=100, bptt_truncate=4):\n",
      "        '''\n",
      "            word_dim   - Vocabulary Size\n",
      "            hidden_dim - Dimension (number of units) of the hidden layer\n",
      "            \n",
      "        '''\n",
      "        # Helper Variables\n",
      "        self.word_dim = word_dim\n",
      "        self.hidden_dim = hidden_dim\n",
      "        self.bptt_truncate = bptt_truncate\n",
      "        \n",
      "        '''\n",
      "            U - Weights(Matrix) from input state  -> hidden state\n",
      "            V - Weights(Matrix) from hidden state -> output state\n",
      "            W - Weights(Matrix) from hidden state -> hidden state (next)\n",
      "        '''\n",
      "        # Randomly Initialize the parameters\n",
      "        U = np.random.uniform(-np.sqrt(6./(word_dim + hidden_dim)), np.sqrt(6./(word_dim + hidden_dim)),\n",
      "                                   (hidden_dim, word_dim))\n",
      "        V = np.random.uniform(-np.sqrt(6./(word_dim + hidden_dim)), np.sqrt(6./(word_dim + hidden_dim)),\n",
      "                                   (word_dim, hidden_dim))\n",
      "        W = np.random.uniform(-np.sqrt(6./(hidden_dim + hidden_dim)), np.sqrt(6./(hidden_dim + hidden_dim)),\n",
      "                                   (hidden_dim, hidden_dim))\n",
      "        # Shared Paramters for theano 'GPU'\n",
      "        self.U = theano.shared(name='U',value=U.astype(theano.config.floatX))\n",
      "        self.V = theano.shared(name='V',value=V.astype(theano.config.floatX))\n",
      "        self.W = theano.shared(name='W',value=W.astype(theano.config.floatX))\n",
      "        # Create the theano Graph\n",
      "        self.theano = {}\n",
      "        self.__theano_build__()\n",
      "    \n",
      "    # Compile the necessary paramters\n",
      "    def __theano_build__(self):\n",
      "        U, V, W = self.U, self.V, self.W\n",
      "        x = tensor.ivector('x')\n",
      "        y = tensor.ivector('y')        \n",
      "        def forward_propagation_step(x_t, hs_prev_t, U, V, W):\n",
      "            hs_t = tensor.tanh(self.U[:, x_t]+ self.W.dot(hs_prev_t))\n",
      "            output_t = tensor.nnet.softmax(self.V.dot(hs_t))\n",
      "            return [output_t[0], hs_t]\n",
      "        [output, hs], updates = theano.scan(forward_propagation_step, sequences=x,\n",
      "                                outputs_info=[None, dict(initial=tensor.zeros(self.hidden_dim))],\n",
      "                                non_sequences=[U, V, W], \n",
      "                                truncate_gradient=self.bptt_truncate, strict=True)\n",
      "        prediction = tensor.argmax(output, axis=1)\n",
      "        output_error = tensor.sum(tensor.nnet.categorical_crossentropy(output, y))\n",
      "        \n",
      "        # Gradients\n",
      "        dU = tensor.grad(output_error, U)\n",
      "        dV = tensor.grad(output_error, V)\n",
      "        dW = tensor.grad(output_error, W)\n",
      "        \n",
      "        # Assign Functions\n",
      "        self.forward_propagation = theano.function([x], output)\n",
      "        self.predict = theano.function([x], prediction)\n",
      "        self.ce_error = theano.function([x, y], output_error)\n",
      "        self.bptt = theano.function([x, y], [dU, dV, dW])\n",
      "        \n",
      "        # Stochastic Gradient Descent\n",
      "        lr = tensor.scalar('lr')\n",
      "        self.sgd_step = theano.function([x, y, lr], [],\n",
      "                        updates=[(self.U, self.U - lr*dU ),\n",
      "                                (self.V, self.V - lr*dV ),\n",
      "                                (self.W, self.W - lr*dW )])\n",
      "        \n",
      "    def calculate_total_loss(self, x, y):\n",
      "        '''\n",
      "            Calculates the total loss for the network predictions\n",
      "        '''\n",
      "        return np.sum([self.ce_error(x_t, y_t) for x_t,y_t in zip(x, y)])\n",
      "    \n",
      "    def calculate_loss(self, x, y):\n",
      "        # Divides the total loss by number of words\n",
      "        num_words = np.sum([len(y_t) for y_t in y])\n",
      "        return self.calculate_total_loss(x, y)/float(num_words)\n",
      "    \n",
      "    \n",
      "    # Stochastic Gradient Descent Optimization\n",
      "    def train_sgd(self, x_tr, y_tr, lr=0.01, nb_epochs=1000, validFreq=5):\n",
      "        '''\n",
      "            x_tr - Training Data Values\n",
      "            y_tr - Training Data Labels\n",
      "            lr   - Learning Rate\n",
      "            nb_epochs - Number of epochs\n",
      "            validFreq - Evaluate the model after this many epochs \n",
      "        '''\n",
      "        # Keeping track of losses, (We'll plot them later)\n",
      "        losses = []\n",
      "        # Keep track of number of sample seen\n",
      "        n_samples_seen = 0\n",
      "        is_loss_evaluated = False\n",
      "        for n_epoch in xrange(nb_epochs):\n",
      "            # Evaluate loss, using validFreq\n",
      "            if n_epoch%validFreq == 0:\n",
      "                loss = self.calculate_loss(x_tr, y_tr)\n",
      "                losses.append((n_samples_seen, loss))\n",
      "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
      "                print time,'Loss after',n_samples_seen,'samples and epoch',n_epoch,'is',loss\n",
      "                # if losses increases, decrease the learning rate\n",
      "                if is_loss_evaluated and loss > losses[-2][1]:\n",
      "                    lr *= 0.5\n",
      "                    print 'Decreasing learning rate to',lr\n",
      "                #sys.stdout.flush()\n",
      "                if not is_loss_evaluated:\n",
      "                    is_loss_evaluated = True\n",
      "            # for each training example, calculate the gradient\n",
      "            for index in range(len(x_tr)):\n",
      "                # Perform one step of SGD\n",
      "                self.sgd_step(x_tr[index], y_tr[index], lr=lr)\n",
      "                n_samples_seen +=1\n",
      "\n",
      "    def save_model_parameters(self, filename):\n",
      "        U, V, W = self.U.get_value(), self.V.get_value(), self.W.get_value()\n",
      "        np.savez(filename, U=U, V=V, W=W)\n",
      "        print 'Saved model parameters to',filename\n",
      "\n",
      "    def load_model_parameters(self, path):\n",
      "        npzfile = np.load(path)\n",
      "        U, V, W = npzfile[\"U\"], npzfile[\"V\"], npzfile[\"W\"]\n",
      "        self.hidden_dim = U.shape[0]\n",
      "        self.word_dim = U.shape[1]\n",
      "        self.U.set_value(U)\n",
      "        self.V.set_value(W)\n",
      "        self.W.set_value(W)\n",
      "        print 'Loaded model parameters from',path,'with hidden dim',U.shape[0],'word dim',U.shape[1]     \n",
      "    \n",
      "    def gradient_check_theano(self, x, y, h=0.001, error_threshold=0.01):\n",
      "        # Overriding the bptt attribute, backprogagte to get the correct gradient\n",
      "        self.bptt_truncate = 250\n",
      "        # Calculate the gradients using backpropagation\n",
      "        bptt_grads = self.bptt(x, y)\n",
      "        # Define the parameters for the model, we want to check\n",
      "        model_parameters =[\"U\", \"V\", \"W\"]\n",
      "        # Check for every parameters\n",
      "        for param_index, param_name in enumerate(model_parameters):\n",
      "            # Get the actual value from the model\n",
      "            tensor_param = operator.attrgetter(param_name)(self)\n",
      "            param = tensor_param.get_value()\n",
      "            print 'Checking Gradients for parameter:',param_name,'with +\\\n",
      "                    shape',param.shape,'and size',np.prod(param.shape)\n",
      "            # Iterating over each parameter of the model\n",
      "            # \u201cmulti_index\u201d causes a multi-index, or a tuple of indices with one per iteration dimension, to be tracked.\n",
      "            # \u201creadwrite\u201d indicates the operand will be read from and written to.\n",
      "            itr = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n",
      "            while not itr.finished:\n",
      "                ml_index = itr.multi_index\n",
      "                # Save the original value, we can reset it later\n",
      "                orig_val = param[ml_index]\n",
      "                # Estimate the gradident (f(x+h)- f(x-h))/(2*h)\n",
      "                param[ml_index] = orig_val + h\n",
      "                tensor_param.set_value(param)\n",
      "                grad_plus = self.calculate_total_loss([x], [y])\n",
      "                param[ml_index] = orig_val - h\n",
      "                tensor_param.set_value(param)\n",
      "                grad_minus = self.calculate_total_loss([x], [y])\n",
      "                estimated_grad = (grad_plus - grad_minus) / (2*h)\n",
      "                # Reset the parameter to the original value\n",
      "                param[ml_index] = orig_val\n",
      "                tensor_param.set_value(param)\n",
      "                backprop_grad = bptt_grads[param_index][ml_index]\n",
      "                # Now, we calculate the relative error: (|a - b|/(|a| + |b|))\n",
      "                relative_error = (np.abs(backprop_grad - estimated_grad)/\n",
      "                                  (np.abs(backprop_grad) + np.abs(estimated_grad)))\n",
      "                # If the error is too high, then backpropgation is wrong,so fail the graident check\n",
      "                if relative_error > error_threshold:\n",
      "                    print 'Gradient Checking for parameter',param_name,'index',ml_index\n",
      "                    print 'Positive loss(f(x+h):',grad_plus\n",
      "                    print 'Negative loss(f(x-h):',grad_minus\n",
      "                    print 'Estimated Gradient:',estimated_grad\n",
      "                    print 'Back Propagation Gradient:',backprop_grad\n",
      "                    print 'Relative Error:',relative_error\n",
      "                    return\n",
      "                itr.iternext()\n",
      "            print 'Gradient Check for parameter',param_name,'passed'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_x, train_y = load_data()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Reading CSV file.. ../data/reddit-comments-2015-08.csv\n",
        "Parsed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 79170 sentences\n",
        "Found"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 65751  unique word tokens\n",
        "Using Vocabulary of size 8000\n",
        "The least frequent word in the vocabulary is \"devoted\" and appeared 10 times\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Random Seed for reproducibility\n",
      "np.random.seed(1234)\n",
      "_VOCABULARY_SIZE = int(os.environ.get('VOCABULARY_SIZE', '8000'))\n",
      "_HIDDEN_DIM = int(os.environ.get('HIDDEN_DIM', '100'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = RNNTheano(word_dim=_VOCABULARY_SIZE, hidden_dim=_HIDDEN_DIM)\n",
      "print \"Checking time for one step of SGD Optimization for sample at index 10\"\n",
      "%timeit model.sgd_step(train_x[10], train_y[10], lr=0.01)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Checking time for one step of SGD Optimization for sample at index 10\n",
        "100 loops, best of 3: 8.92 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Checking SGD Optimization for 100 samples\"\n",
      "%timeit losses = model.train_sgd(train_x[:100], train_y[:100], lr=0.01, nb_epochs=100, validFreq=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Checking SGD Optimization for 100 samples\n",
        "2016-12-23 13:40:39"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 0 samples and epoch 0 is 10.8617120048\n",
        "2016-12-23 13:40:45"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 1000 samples and epoch 10 is 7.21892458101\n",
        "2016-12-23 13:40:51"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 2000 samples and epoch 20 is 7.16365580561\n",
        "2016-12-23 13:40:57"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 3000 samples and epoch 30 is 7.117124016\n",
        "2016-12-23 13:41:03"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 4000 samples and epoch 40 is 7.32226066531\n",
        "Decreasing learning rate to 0.005\n",
        "2016-12-23 13:41:08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 5000 samples and epoch 50 is 7.27030396934\n",
        "2016-12-23 13:41:14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 6000 samples and epoch 60 is 7.36144666154\n",
        "Decreasing learning rate to 0.0025\n",
        "2016-12-23 13:41:20"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 7000 samples and epoch 70 is 7.3694565571\n",
        "Decreasing learning rate to 0.00125\n",
        "2016-12-23 13:41:26"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 8000 samples and epoch 80 is 7.22871500444\n",
        "2016-12-23 13:41:32"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 9000 samples and epoch 90 is 7.27737846781\n",
        "Decreasing learning rate to 0.000625\n",
        "2016-12-23 13:41:37"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 0 samples and epoch 0 is 7.30597751476\n",
        "2016-12-23 13:41:43"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 1000 samples and epoch 10 is 7.61561379111\n",
        "Decreasing learning rate to 0.005\n",
        "2016-12-23 13:41:49"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 2000 samples and epoch 20 is 7.48769303104\n",
        "2016-12-23 13:41:55"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 3000 samples and epoch 30 is 7.67478683262\n",
        "Decreasing learning rate to 0.0025\n",
        "2016-12-23 13:42:01"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 4000 samples and epoch 40 is 7.56805187437\n",
        "2016-12-23 13:42:07"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 5000 samples and epoch 50 is 7.63837082196\n",
        "Decreasing learning rate to 0.00125\n",
        "2016-12-23 13:42:13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 6000 samples and epoch 60 is 7.62535610557\n",
        "2016-12-23 13:42:18"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 7000 samples and epoch 70 is 7.6722291217\n",
        "Decreasing learning rate to 0.000625\n",
        "2016-12-23 13:42:24"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 8000 samples and epoch 80 is 7.70609238113\n",
        "Decreasing learning rate to 0.0003125\n",
        "2016-12-23 13:42:30"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 9000 samples and epoch 90 is 7.73899395712\n",
        "Decreasing learning rate to 0.00015625\n",
        "2016-12-23 13:42:36"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 0 samples and epoch 0 is 7.74060585561\n",
        "2016-12-23 13:42:42"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 1000 samples and epoch 10 is 8.05362910265\n",
        "Decreasing learning rate to 0.005\n",
        "2016-12-23 13:42:48"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 2000 samples and epoch 20 is 7.89246454815\n",
        "2016-12-23 13:42:53"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 3000 samples and epoch 30 is 8.2869744715\n",
        "Decreasing learning rate to 0.0025\n",
        "2016-12-23 13:42:59"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 4000 samples and epoch 40 is 8.14455406456\n",
        "2016-12-23 13:43:05"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 5000 samples and epoch 50 is 8.2256335505\n",
        "Decreasing learning rate to 0.00125\n",
        "2016-12-23 13:43:11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 6000 samples and epoch 60 is 8.22722362636\n",
        "Decreasing learning rate to 0.000625\n",
        "2016-12-23 13:43:17"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 7000 samples and epoch 70 is 8.23346142553\n",
        "Decreasing learning rate to 0.0003125\n",
        "2016-12-23 13:43:23"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 8000 samples and epoch 80 is 8.22198024457\n",
        "2016-12-23 13:43:29"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 9000 samples and epoch 90 is 8.22260268537\n",
        "Decreasing learning rate to 0.00015625\n",
        "2016-12-23 13:43:34"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 0 samples and epoch 0 is 8.22516932374\n",
        "2016-12-23 13:43:40"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 1000 samples and epoch 10 is 8.42965179025\n",
        "Decreasing learning rate to 0.005\n",
        "2016-12-23 13:43:46"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 2000 samples and epoch 20 is 8.30472023394\n",
        "2016-12-23 13:43:52"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 3000 samples and epoch 30 is 8.41032388744\n",
        "Decreasing learning rate to 0.0025\n",
        "2016-12-23 13:43:58"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 4000 samples and epoch 40 is 8.52994064246\n",
        "Decreasing learning rate to 0.00125\n",
        "2016-12-23 13:44:04"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 5000 samples and epoch 50 is 8.56937710291\n",
        "Decreasing learning rate to 0.000625\n",
        "2016-12-23 13:44:09"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 6000 samples and epoch 60 is 8.58586806437\n",
        "Decreasing learning rate to 0.0003125\n",
        "2016-12-23 13:44:15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 7000 samples and epoch 70 is 8.60180949245\n",
        "Decreasing learning rate to 0.00015625\n",
        "2016-12-23 13:44:21"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 8000 samples and epoch 80 is 8.61387046089\n",
        "Decreasing learning rate to 7.8125e-05\n",
        "2016-12-23 13:44:27"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 9000 samples and epoch 90 is 8.61304814468\n",
        "1 loops, best of 3: 58.3 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Checking SGD Optimization for all the samples\"\n",
      "%timeit losses = model.train_sgd(train_x, train_y, lr=0.01, nb_epochs=60, validFreq=100)\n",
      "model.save_model_parameters('../data/my_trained_rnn_theano.npz')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Checking SGD Optimization for all the samples\n",
        "2016-12-23 19:07:57"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 0 samples and epoch 0 is 10.0922567179\n",
        "2016-12-24 02:36:25"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 0 samples and epoch 0 is 6.26135921452\n",
        "2016-12-24 10:04:57"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Loss after 0 samples and epoch 0 is 6.47961324823\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-41-6e7cb15225ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Checking SGD Optimization for all the samples\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'timeit losses = model.train_sgd(train_x, train_y, lr=0.01, nb_epochs=60, validFreq=100)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/my_trained_rnn_theano.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2203\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2204\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2207\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2124\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2125\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2126\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2127\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/lib/python2.7/dist-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell)\u001b[0m\n",
        "\u001b[0;32m/usr/lib/python2.7/dist-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/lib/python2.7/dist-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m   1014\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m         \u001b[0mall_runs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m         \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mquiet\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/lib/python2.7/timeit.pyc\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(self, repeat, number)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/lib/python2.7/timeit.pyc\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
        "\u001b[0;32m<ipython-input-32-f5f2faa4e0be>\u001b[0m in \u001b[0;36mtrain_sgd\u001b[0;34m(self, x_tr, y_tr, lr, nb_epochs, validFreq)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0;31m# Perform one step of SGD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgd_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0mn_samples_seen\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    949\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    950\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 951\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    938\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                         self, node)\n\u001b[0m\u001b[1;32m    941\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Global Variables\n",
      "VOCABULARY_SIZE = 8000\n",
      "UNKNOWN_TOKEN = \"UNKNOWN_TOKEN\"\n",
      "SENTENCE_START_TOKEN = \"SENTENCE_START_TOKEN\"\n",
      "SENTENCE_END_TOKEN = \"SENTENCE_END_TOKEN\"\n",
      "\n",
      "sentences = read_data()\n",
      "# Tokenize the sentences into words\n",
      "tok_sentences = np.array([nltk.word_tokenize(sentence) for sentence in sentences])\n",
      "\n",
      "# Count the word frequencies\n",
      "word_freq = nltk.FreqDist(itertools.chain(*tok_sentences))\n",
      "\n",
      "# Getting the most common words and build index_to_word and word_to_index vectors\n",
      "vocabulary = np.array(word_freq.most_common(VOCABULARY_SIZE - 1))\n",
      "index_to_word = [row[0] for row in vocabulary]\n",
      "index_to_word.append(UNKNOWN_TOKEN)\n",
      "word_to_index = dict([(word, index) for(index,word) in enumerate(index_to_word)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Reading CSV file.. ../data/reddit-comments-2015-08.csv\n",
        "Parsed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 79170 sentences\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def generate_sentences(model):    \n",
      "    # Starts with sentence token\n",
      "    new_sentence = [word_to_index[SENTENCE_START_TOKEN]]\n",
      "    # Until we get the end token\n",
      "    while not new_sentence[-1] == word_to_index[SENTENCE_END_TOKEN]:\n",
      "        prob_next_word = model.forward_propagation(new_sentence)\n",
      "        sample_word = word_to_index[UNKNOWN_TOKEN]\n",
      "        # We wish to remove this sample word\n",
      "        while sample_word == word_to_index[UNKNOWN_TOKEN]:\n",
      "            samples = np.random.multinomial(1, prob_next_word[-1])\n",
      "            sample_word = np.argmax(samples)\n",
      "        new_sentence.append(sample_word)\n",
      "    sentence_string = [index_to_word[x] for x in new_sentence[1:-1]]\n",
      "    return sentence_string"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = RNNTheano(word_dim=_VOCABULARY_SIZE, hidden_dim=50)\n",
      "print 'Loading parameters for the model'\n",
      "model.load_model_parameters('../data/trained-model-theano.npz')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loading parameters for the model\n",
        "Loaded model parameters from ../data/trained-model-theano.npz with hidden dim 50 word dim 8000\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_sentences = 15\n",
      "min_length = 8\n",
      "for x in range(num_sentences):\n",
      "    sent = []\n",
      "    # We want sentences with atleast min length\n",
      "    while len(sent) < min_length:\n",
      "        sent = generate_sentences(model)\n",
      "    print \" \".join(sent)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "would ] for my '' be my `` would just have in '' '' be do you ) ) my you for my '' ) , in '' it be have you was have it not and my '' my SENTENCE_START_TOKEN he to be it of it i my : my ) my '' in it ( was was he ) ] be like on . be have : be SENTENCE_START_TOKEN this but just you or ) ) 's a can your if my '' ) your '' ) you he ) `` my '' be not it ; do you my ) would you my my ) ) of , you my it : be ] . be ) ) it of ) ) , but in ) ? not my that on ( if 's but this ) ) not ( that ) ) is to . it so do ( be '' do you my if my that my ( and this SENTENCE_START_TOKEN ) so so but are just\n",
        "( as my : be and was : be to ; be as your '' not it or that ) you [ '' can 's\n",
        "SENTENCE_START_TOKEN a : do '' ) ) it you ; my so : the ( be ] a : if you my ) [ was ( as a he my '' '' ) my do you my '' , a ) like you ( do 's '' ) they you my ) be be '' would you it a ) for it you are '' would but , '' , was SENTENCE_START_TOKEN you as my was in be you my that my or , have , like `` my you [ would this but to it , '' can and `` like have ; be have the '' your are SENTENCE_START_TOKEN the that ) ) , ( do my do just ; my '' my do my ? be in ) it that '' in '' '' ) ) it you you my be do would ) it it ( '' ) this . be or you `` it can ; ( be my '' ) ) are , my '' ) ? be SENTENCE_START_TOKEN a so ? it my that would but 's are : be for with they of my '' ) ) ) a he be my '' are a in ? be ? on that ) ? the in my in '' ) it ) do he it ( to ) you you my '' ) but be '' that my ( on `` be '' '' you you my of SENTENCE_START_TOKEN"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "; ? the ) of a you ; . be like be was ) my\n",
        "the '' '' ) , it ) ) you would just my '' ) ? this this ) my '' : be would this have ; it ) but you do ( have . be 's he to my my '' '' ) have my be '' so of would he to ) my '' '' in are you my in it at SENTENCE_START_TOKEN , my '' ) ] , to my as a he my at ) my my do you my '' my ? be my '' '' )"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "do can your ) ) it '' '' my '' my ? this have in ) my '' my a you my ) my as that so but can that ) ) was , '' not '' '' SENTENCE_START_TOKEN but do"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "SENTENCE_START_TOKEN as can as ( your ) but `` my '' can in `` a or have ) it that can have it my '' ) ( be that ) ( do '' ) but but but ( would with my '' ) my he my be it do ["
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "at was n't '' SENTENCE_START_TOKEN ( ) ) my ) , is can he ) ) , so would just ( `` a can '' in ( i and my '' my have on my '' my [ ( be ) have '' ) are that my '' ) ? be you it for can"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "are and ? the do you and my ) . it you ? would do do on ? be a '' '' be with ? he ) to it but my my my SENTENCE_START_TOKEN and my '' my : my my '' ) ( that my was '' ) that it just and '' ( and my '' ) and n't , so like `` not ; be : be"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "you ) my : be that '' '' ) ) my SENTENCE_START_TOKEN '' . be the can he so and a '' this ) so like in ( so have if that you but : have be ) ) it my ( ) my '' ) is but would your you do 's `` a you are ( be `` my 's be a just '' in at ) is a '' my that my '' '' do you my '' on my '' ) : be ? that ) ) ? be my '' can `` in my , you would and for just a ( 's you my ) are but not and '' '' ? : be my '' my that that ) ) it in it my ( can ) SENTENCE_START_TOKEN can to it ) ? be my that are he )"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "or , my '' ) it you can but `` my '' my ) of '' are to my '' be it be : ) . be can at you be is you my ) are like '' my '' would"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "would do ( a SENTENCE_START_TOKEN ; have have ( like in 's but `` be would be it so a do '' '' he be '' '' be '' they you my . be a can you ; be in can it my '' are it this a ) my it or can ( be a he ) be ( ? and in have my '' ) ) it can can ) 's be a '' ) it it that [ ? be like on of just : with ? are a ) it '' '' you 's but '' have that have so or '' ) be that are be a ) are [ '' would [ ) 's ( it ) ) in it '' if my and my my : my '' are my '' but . be he be he ) , my ) [ '' you would that you so ( '' ) ) to this you my '' ? this this be so ) are"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "the he my my but can and my that my SENTENCE_START_TOKEN ) and my '' ) ) my of do you it but ? on be 's a ) this you you my ] a ; not '' are : have :"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "do [ '' they a not on ( '' ) ) that a SENTENCE_START_TOKEN he my '' ) be it so ) it ) are ) a '' it ) it you at ) he : this this be you you are you he and my the if '' ) be it be and so . be my can the ( ; your my are `` in ? be ? the SENTENCE_START_TOKEN '' ) you my '' ) my my '' ? be my '' can this , you it ] [ ) this is you my '' they was , ? be not it '' ; be 's '' : with my `` my in '' it my '' '' a on . be : a ] be ''"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "would can at can in ; be my '' ) my you . be it your '' ) my ? on my he so ) and '' my '' my ( . be like with '' '' ( be a ) you just my as would my in ; be my '' ) ) it you my ? be be [ ( on have do a : the '' my a and ) my you you my ; n't you that SENTENCE_START_TOKEN it but my be '' ) your '' that ) SENTENCE_START_TOKEN he not that ( ? a my as ) the ( and my '' ) you , my"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}